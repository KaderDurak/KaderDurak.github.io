<!doctype html>
<html lang="tr"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Decision Tree Algorithm - KADER DURAK BLOG</title><meta description="What is a Decision Tree?Karar Ağacı, sınıflandırma problemlerinde yaygın olarak kullanılan önceden tanımlanmış bir hedef değişkene sahip ve denetlenen(supervised) öğrenme algoritması olarak tanımlanab"><meta property="og:type" content="article"><meta property="og:title" content="Decision Tree Algorithm"><meta property="og:url" content="https://kaderdurak.github.io/DecisionTree/"><meta property="og:site_name" content="KADER DURAK BLOG"><meta property="og:description" content="What is a Decision Tree?Karar Ağacı, sınıflandırma problemlerinde yaygın olarak kullanılan önceden tanımlanmış bir hedef değişkene sahip ve denetlenen(supervised) öğrenme algoritması olarak tanımlanab"><meta property="og:locale" content="tr_TR"><meta property="og:image" content="https://kaderdurak.github.io/DecisionTree/EoapOPlXcAE_XPa.jpg"><meta property="article:published_time" content="2021-01-11T19:11:38.000Z"><meta property="article:modified_time" content="2021-01-11T19:12:29.120Z"><meta property="article:author" content="Kader Durak"><meta property="article:tag" content="Decision Tree"><meta property="article:tag" content="Information Gain"><meta property="article:tag" content="Gini Index"><meta property="article:tag" content="Entropy"><meta property="article:tag" content="Gini Impurity"><meta property="article:tag" content="Pruning Tree"><meta property="article:tag" content="Cost Complexity Pruning"><meta property="article:tag" content="Reduced Error Pruning"><meta property="article:tag" content="Minimum Error Pruning (MEP)"><meta property="article:tag" content="Pessimistic Pruning"><meta property="article:tag" content="Error-Based Pruning (EBP)"><meta property="article:tag" content="Minimum Description Length (MDL) Pruning"><meta property="article:tag" content="Iterative Dichotomiser 3 (ID3)"><meta property="article:tag" content="C4.5"><meta property="article:tag" content="Classification and Regression Tree (CART)"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/DecisionTree/EoapOPlXcAE_XPa.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kaderdurak.github.io/DecisionTree/"},"headline":"KADER DURAK BLOG","image":["https://kaderdurak.github.io/DecisionTree/EoapOPlXcAE_XPa.jpg"],"datePublished":"2021-01-11T19:11:38.000Z","dateModified":"2021-01-11T19:12:29.120Z","author":{"@type":"Person","name":"Kader Durak"},"description":"What is a Decision Tree?Karar Ağacı, sınıflandırma problemlerinde yaygın olarak kullanılan önceden tanımlanmış bir hedef değişkene sahip ve denetlenen(supervised) öğrenme algoritması olarak tanımlanab"}</script><link rel="canonical" href="https://kaderdurak.github.io/DecisionTree/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KADER DURAK BLOG" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Anasayfa</a><a class="navbar-item" href="/archives">Arşivler</a><a class="navbar-item" href="/categories">Katergoriler</a><a class="navbar-item" href="/tags">Etiketler</a><a class="navbar-item" href="/about">Hakkımda</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="/DecisionTree/EoapOPlXcAE_XPa.jpg" alt="Decision Tree Algorithm"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-01-11T19:11:38.000Z" title="2021-01-11T19:11:38.000Z">2021-01-11</time><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">41 dakika read (About 6193 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Decision Tree Algorithm</h1><div class="content"><h3 id="What-is-a-Decision-Tree"><a href="#What-is-a-Decision-Tree" class="headerlink" title="What is a Decision Tree?"></a>What is a Decision Tree?</h3><p>Karar Ağacı, sınıflandırma problemlerinde yaygın olarak kullanılan önceden tanımlanmış bir hedef değişkene sahip ve denetlenen(supervised) öğrenme algoritması olarak tanımlanabilir. Karar Ağaçları yapısı gereği sınıflandırma problemlerine mükemmel uyar.<a id="more"></a></p>
<p>Karar ağaçları hem sürekli girdi-çıktı değişkenlerinde hem de kategorik çıktı değişkenlerinde kullanılabilir. Bu yöntemde örnek, giriş değişkenlerindeki en önemli ayırıcı veya farklılaştırıcı tarafından iki veya daha fazla homojen kümeye bölünür.</p>
<p>Örnek olarak 3 farklı değişkeni olan 30 tane çocuğun olduğu bir gruba bakalım. </p>
<ul>
<li>Gender (M,F)</li>
<li>Town (A ,B)</li>
<li>Weight(50-60 kg)</li>
</ul>
<p>15 tanesi boş zamanlarında <strong>Lord of The Rings(LOTR)</strong>  izliyor. Şimdi şart, boş zamanlarında kimin LOTR izlediğini tahmin edecek bir model yaratmak istemem. Bu problem için çocukları farklılaştırmak ve LOTR izleyen çocuklar gibi kategorilere eklemek gerekir. Bu, üçü arasında oldukça önemli bir girdi değişkenine dayanmalıdır.<br>Böyle bir durumda, bir karar ağacı, üç değişkeni de kullanarak çocukları farklılaştırdığı ve sınıflandırdığı için çok yardımcı olabilir ve birbirlerine heterojen olacak en homojen çocuk gruplarını oluşturur.</p>
<p>Karar ağaçları, yukarıda bahsedildiği gibi, en önemli değişkeni ve onun değerini bulur ve tanımlar. Yani en iyi homojen veri kümesini bulur. Bununla birlikte buradan çıkan soru, bunu nasıl yaptığı yani değişkeni ve bölünmeyi nasıl tanımladığıdır. Karar ağacı, bu eylemi gerçekleştirmek için çeşitli algoritmalar kullanır.</p>
<h3 id="Types-of-Decision-Trees"><a href="#Types-of-Decision-Trees" class="headerlink" title="Types of Decision Trees"></a>Types of Decision Trees</h3><p>Karar ağacının sınıflandırılması, mevcut olan hedef değişken türüne bağlı olabilir. İki tipte olabileceği bulunmuştur:</p>
<h4 id="1-Categorical-Variable-Decision-Tree"><a href="#1-Categorical-Variable-Decision-Tree" class="headerlink" title="1. Categorical Variable Decision Tree:"></a>1. Categorical Variable Decision Tree:</h4><p>Kategorik hedef değişkeni olan Karar ağaçlarıdır. Az önce bahsettiğimiz örneğe dönersek, içindeki hedef değişken” Çocuklar LOTR izliyor mu?” cevap EVET veya HAYIR.</p>
<h4 id="2-Continuous-Variable-Decision-Tree"><a href="#2-Continuous-Variable-Decision-Tree" class="headerlink" title="2. Continuous Variable Decision Tree:"></a>2. Continuous Variable Decision Tree:</h4><p>Karar ağacında devam eden bir hedef değişkeni vardır. Örneğin bir kişinin vergi departmanına ödeme yapacağını varsayalım(Evet/Hayır). Burada bireyin geliriniin önemli bir değişken olduğu görülmektedir. Ancak vergi şirketi, kişinin ayrıntılarına sahip olabilir veya olmayabilir. Şimdi, değişkenin öneminin zaten farkında olduğumuz için, ürün, meslek vb. değişkenler kullanılarak geliri tahmin etmek için bir karar ağacı oluşturulabilir. Böyle bir durumda, sürekli bir değişken için tahminler gerçekleşiyor.</p>
<p>Karar Ağacı genellikle ağaç metaforlarıyla dolu bir jargonu vardır. En önemli deyimlere bir göz atalım:<br><img src="/DecisionTree/1.jpg"><br><strong>Root Node</strong>:Toplam örnek veya popülasyon anlamına gelir. Bu daha sonra iki veya daha fazla homojen kümeye bölünür.<br><strong>Splitting</strong>: Düğümleri(nodes) iki veya daha fazla alt düğümlere(sub nodes) ayırmak için kullanılan işlemdir.<br><strong>Decision Node</strong>: Alt düğümlerin daha fazla alt düğümlere bölünmesi splitting olarak bilinir. Ancak, bu süreçte oluşturulan düğüm bir karar düğümü olarak bilinir.<br><strong>Leaf/Terminal Node</strong>: Yaprak veya terminal olarak bilinen bölünemeyen düğümlerdir.<br><strong>Pruning</strong>:Ağaç ve bitkiler metaforlarına devam edersek, bir karar düğümünün alt düğümlerinin kaldırıldığı süreç, budama olarak bilinir. Dolayısıyla bölünmeyi önleyen bir süreçtir.<br><strong>Branch / Sub-Tree</strong>:Tam bir ağacın alt bölümü, alt ağaç veya dal olarak bilinir.<br><strong>Parent and Child Node</strong>:Düğümlerin bölümünde, bölünen düğümler parent node olarak adlandırılırken, child node bu bölünme nedeniyle oluşur.</p>
<p>Karar ağaçlarında, bir kayıt için bir sınıf etiketi tahmin etmek için ağacın kökünden(root) başlarız. Kök niteliğinin değerlerini kaydın niteliğiyle karşılaştırırız. Karşılaştırma temelinde, bu değere karşılık gelen dalı izler ve bir sonraki düğüme atlarız.</p>
<p>Tahmin edilen sınıf değerine sahip bir yaprak düğüme(leaf node) ulaşana kadar kaydımızın nitelik değerlerini ağacın diğer dahili düğümleriyle karşılaştırmaya devam ediyoruz.<br>Şimdi karar ağacı modelini nasıl oluşturabileceğimizi anlayalım.</p>
<p>Karar ağacını kullanırken yaptığımız varsayımlardan bazıları aşağıdadır:</p>
<ul>
<li>Başlangıçta tüm eğitim seti kök(root) olarak kabul edilir.</li>
<li>Özellik değerlerinin kategorik olması tercih edilir. Değerler süreklilik arz ediyorsa modeli oluşturmadan önce ayrıklaştırılır.</li>
<li>Kayıtlar, nitelik değerlerine göre yinelemeli olarak dağıtılır.</li>
<li>Ağacın kök veya iç düğümü olarak niteliklerin yerleştirilmesi, bazı istatistiksel yaklaşımlar kullanılarak yapılır.</li>
</ul>
<p>Karar ağacı uygulamasındaki birincil zorluk, hangi nitelikleri kök düğüm ve her düzeyde kök düğüm(root) olarak ele almamız gerektiğini belirlemektir. Buna <strong>nitelik seçimi(attributes selection)</strong> denir. Her seviyede kök düğüm olarak düşünülebilecek niteliği belirlemek için farklı nitelik seçim ölçülerimiz vardır.</p>
<p>Nitelik seçiminde kullanılan popüler ölçümler:</p>
<ul>
<li>Information Gain</li>
<li>Gini Index</li>
</ul>
<h4 id="Attributes-Selection"><a href="#Attributes-Selection" class="headerlink" title="Attributes Selection"></a>Attributes Selection</h4><p>Veri kümesi “n” nitelikten oluşuyorsa, hangi niteliğin kökte veya ağacın farklı düzeylerinde dahili düğümler olarak yerleştirileceğine karar vermek karmaşık bir adımdır. Kök olacak herhangi bir düğümü rastgele seçmek sorunu çözemez. Rastgele bir yaklaşım izlersek, bize düşük doğrulukta kötü sonuçlar verebilir.</p>
<p>Bu nitelik seçme problemini çözmek için araştırmacılar Information Gain, Gini Indeksi gibi çözümler geliştirdiler. Bunlar her özellik için değerleri hesaplayacaktır. Değerler sıralanır ve nitelikler sıraya göre ağaca yerleştirilir, yani yüksek değerli nitelik (Information Gain durumunda) köke yerleştirilir.</p>
<p>Information Gain’i bir kriter olarak kullanırken, niteliklerin kategorik olduğunu varsayılır. Gini indeksi için niteliklerin sürekli olduğu varsayılır.</p>
<h4 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h4><p>Information Gain bir kriter olarak kullanarak, her bir özelliğin içerdiği bilgileri tahmin etmeye çalışırız. </p>
<p><strong>Entropi</strong>, rastgele bir değişkenin belirsizliğinin ölçüsüdür, rastgele bir örnek koleksiyonunun safsızlığını karakterize eder. Entropi ne kadar yüksekse bilgi içeriği o kadar fazladır. Veri biliminde entropi, bir sütunun ne kadar “karışık” olduğunu ölçmenin bir yolu olarak kullanılır. Özellikle, düzensizliği ölçmek için entropi kullanılır.</p>
<p>Yalnızca iki sınıflı binary classification problemi için, pozitif ve negatif sınıf vardır.</p>
<ul>
<li>Tüm örnekler pozitifse veya tümü negatifse, o zaman entropi 0, yani düşük olacaktır.</li>
<li>Kayıtların yarısı pozitif sınıfta ve yarısı negatif sınıfta ise, o zaman entropi 1 yani yüksektir.</li>
</ul>
<p>Çıkarılacak sonuç sınıflandırdığımız dalların entropisinin düşük olması yani homojen olmasını isteriz.</p>
<p>Entropi formülü:<br><img src="/DecisionTree/3.jpg"><br>Bir örnekle entropi hesaplayalım. Diyelim ki aşağıdaki görselde olduğu gibi bir veri setimiz olsun.<br><img src="/DecisionTree/4.jpg"></p>
<p>Eğer x= 1.5 için verileri ayırırsak nele oluyor bakalım.<br><img src="/DecisionTree/5.jpg"></p>
<p>Bu ayırma işlemi veriyi iki dala veya kümeye ayıracak. Bu dallarda</p>
<ul>
<li>Sol dalda, 4 mor </li>
<li>Sağ dalda, 1 mor, 5 yeşil </li>
</ul>
<p>Bu ayırma optimal mi, en iyi mi? Gelin bu ayırmayı en iyi olacak şekilde ölçelim.<br>C tane sınıf için entropi formülü:<br><img src="/DecisionTree/6.jpg"><br>Ayırmadan önce 5 mor ve 5 yeşil verimiz vardı.Ayırma öncesi entropi Ebefore:<br><img src="/DecisionTree/7.jpg"></p>
<p>Ayırma işleminden sonra entropilerimiz Eleft ve Eright:<br><img src="/DecisionTree/8.jpg"></p>
<p>Ayırma entropimiz Esplit:<br><img src="/DecisionTree/9.jpg"></p>
<p>Her özelliğin entropi ölçüsünü hesaplayarak information gain hesaplanabilir. Information Gain, niteliğe göre sıralama nedeniyle entropide beklenen azalmayı hesaplar.</p>
<p>Information Gain hesaplayalım. Burda ne kadar entropiden kurtulduğumuzu buluruz. Parent entropimiz Ebefore, child entropimiz Esplit.<br>IG (Parent,Child) = E(Parent)- E(Child)</p>
<p><img src="/DecisionTree/10.jpg"></p>
<h4 id="Gini-Index"><a href="#Gini-Index" class="headerlink" title="Gini Index"></a>Gini Index</h4><p>İtalyan istatistikçi ve sosyolojist olan Corrado Gini, toplumda gelir eşitsizliğini ölçmek için geliştirdiği  gini indeksine bakalım.<br><strong>Gini İndeksi</strong>, rastgele seçilen bir öğenin ne sıklıkla yanlış bir şekilde tanımlanacağını ölçmek için kullanılan bir metriktir. Daha düşük gini indeksli bir niteliğin tercih edilmesi gerektiği anlamına gelir.</p>
<p>Şimdi görseldeki gibi veri setimiz olsun. Gini indeksini ölçelim.</p>
<p><img src="/DecisionTree/11.jpg"></p>
<p>x= 2 için verisetini ayıralım.</p>
<p><img src="/DecisionTree/12.jpg"><br>Bu mükemmel bir ayrım! Veri kümemizi mükemmel şekilde iki kola ayırır:</p>
<ul>
<li>Sol dalda, 5 mor </li>
<li>Sağ dalda, 5 yeşil </li>
</ul>
<p>Ya biz x= 1.5 değerinde ayırırsak?<br><img src="/DecisionTree/5.jpg"></p>
<ul>
<li>Sol dalda, 4 mor </li>
<li>Sağ dalda, 1 mor, 5 yeşil </li>
</ul>
<p>Hangi ayırmanın daha iyi olduğunu nasıl anlarız?<br>İşte bu noktada Gini Impurity bize yolu gösteriyor.</p>
<ol>
<li>Veri kümemizde rastgele bir veri noktası seçin</li>
<li>Veri kümesindeki sınıf dağılımına göre rastgele sınıflandırın.</li>
</ol>
<p>Veri noktasını yanlış sınıflandırmamızın olasılığı nedir? Bu sorunun cevabı <strong>Gini Impurity</strong>.</p>
<p>Veri kümemizin tamamının Gini Impurity’sini hesaplayalım. Rasgele bir veri noktası seçersek, bu ya mor (%50) veya yeşil (%50)dir.</p>
<p>Şimdi, veri noktamızı sınıf dağılımına göre rastgele sınıflandırıyoruz. Her bir rengin 5’ine sahip olduğumuz için %50 oranında mor, %50 oranında yeşil olarak sınıflandırıyoruz.</p>
<p>Veri noktamızı yanlış sınıflandırmamızın olasılığı nedir?</p>
<p><img src="/DecisionTree/13.jpg"></p>
<p>Yukarıda sadece 2 olayı yanlış sınıflandırdık. Toplam olasılığımız </p>
<p>25%+25% =50%<br>Gini Impurity = 0.5</p>
<p>C tane sınıf için Gini Impurity formülü:</p>
<p><img src="/DecisionTree/14.jpg"></p>
<p>C=2 (mor,yeşil)  P(1)=p(2)=0.5</p>
<p>G = p(1)∗(1−p(1))+p(2)∗(1−p(2))<br>G = 0.5∗(1−0.5)+0.5∗(1−0.5)<br>G = 0.5<br>Tabloda hesapladığımız sonuç çıktı.<br>​</p>
<p>Yaptığımız mükemmel bölüme geri dönelim. Bölünmeden sonra iki dalın Gini Impurity değerleri nelerdir?<br><img src="/DecisionTree/12.jpg"><br>Sol dalda sadece mor veriler var.<br>Gini Impurity değeri:<br>Gleft = 1∗(1−1) + 0∗(1−0) = 0<br>​<br>Sağ dalda sadece yeşil veriler var.<br>​Gini Impurity değeri:<br>Gright = 0∗(1−0) + 1∗(1−1) = 0<br>​<br>Gini Gain = 0.5 - 0 = 0.5 (for x=2)<br>Her iki dalda da 0 safsızlık var. Mükemmel ayrım, 0.5 safsızlık içeren bir veri kümesini 0 safsızlık içeren 2 dala dönüştürdü.</p>
<p>0 değerindeki bir Gini Impurity, mümkün olan en düşük ve en iyi safsızlıktır. Sadece her şey aynı sınıf olduğunda elde edilebilir.</p>
<p>Şimdi x=1.5 için ayırdığımız ver kümesine bakalım.<br><img src="/DecisionTree/5.jpg"><br>Sol dalda sadece mor veriler var.<br>Gini Impurity değeri:<br>Gleft = 1∗(1−1) + 0∗(1−0) = 0</p>
<p>Sağ dalda 1 mor ve 5 yeşil veri var.<br>​Gini Impurity değeri:<br>Gright = 1/6∗(1−1/6) + 5/6∗(1−5/6)<br>Gright = 0.278</p>
<p>Her dalın safsızlığını kaç veriye sahip olduğuna göre ağırlıklandırarak ayrımın kalitesini belirleyeceğiz. Sol dalda 4 veriye ve Sağ dalda 6 veriye sahip olduğundan, şunu elde ederiz:<br>G = (0.4∗0) + (0.6∗0.278) = 0.167</p>
<p>Bu ayırımla çıkardığımız safsızlık miktari yani kazanç(gain):</p>
<p>Gini Gain = 0.5- 0.67 = 0.33 (for x= 1.5)</p>
<p>0.5 &gt; 0.33’ten büyük olduğu için x=2 noktası en iyi ayırım noktasıdır.<br>Büyük Gini Gain=  En İyi Ayırım  </p>
<p>Bir karar ağacı eğitilirken, dalların ağırlıklı safsızlıklarının orijinal safsızlıktan çıkarılmasıyla hesaplanan Gini Gain maksimize ederek en iyi ayrım seçilir.</p>
<h4 id="Entropy-vs-Gini-Impurity"><a href="#Entropy-vs-Gini-Impurity" class="headerlink" title="Entropy vs Gini Impurity"></a>Entropy vs Gini Impurity</h4><p><img src="/DecisionTree/15.jpg"><br>Her iki yöntemin de dahili çalışması çok benzerdir ve her ikisi de her yeni bölünmeden sonra feature/split hesaplamak için kullanılır. Ancak her iki yöntemi de karşılaştırırsak, o zaman Gini Impurity bilgi işlem gücü açısından entropiden daha verimlidir. Entropi grafiğinde de görebileceğiniz gibi, önce 1’e kadar artar ve sonra azalmaya başlar, ancak Gini Impurity durumunda sadece 0,5’e kadar gider ve sonra azalmaya başlar, dolayısıyla daha az hesaplama gücü gerektirir. Entropi aralığı 0 ila 1 arasındadır ve Gini Impurity aralığı 0 ila 0,5 arasındadır. Bu nedenle, Gini Impurity’nin  en iyi özellikleri seçmek için entropiye kıyasla daha iyi olduğu sonucuna varabiliriz.</p>
<h3 id="Pruning-Tree"><a href="#Pruning-Tree" class="headerlink" title="Pruning Tree"></a>Pruning Tree</h3><h4 id="Stopping-Criteria"><a href="#Stopping-Criteria" class="headerlink" title="Stopping Criteria"></a>Stopping Criteria</h4><p>Büyüme aşaması, bir durdurma kriteri tetiklenene kadar devam eder. Aşağıdaki koşullar ortak durdurma kurallarıdır:</p>
<ol>
<li>Eğitim setindeki tüm örnekler tek bir y değerine aittir.</li>
<li>Maksimum ağaç derinliğine ulaşıldı.</li>
<li>Terminal düğümündeki vaka sayısı, üst düğümler için minimum vaka sayısından azdır.</li>
<li>Düğüm bölünmüşse, bir veya daha fazla alt düğümdeki vaka sayısı, alt düğümler için minimum vaka sayısından daha az olacaktır.</li>
<li>En iyi bölme kriteri, belirli bir eşikten büyük değildir.</li>
</ol>
<h4 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h4><p><img src="/DecisionTree/17.jpg"><br>Sıkı durdurma kriterlerinin kullanılması, küçük ve yetersiz donatılmış karar ağaçları oluşturma eğilimindedir. Öte yandan, gevşek durdurma kriterlerinin kullanılması, eğitim setine aşırı uygun(overfit) büyük karar ağaçları oluşturma eğilimindedir. Bu ikilemi çözmek için, gevşek bir durdurma kriterine dayalı ve karar ağacının eğitim setini aşırı öğrenmesine(overfit) izin veren bir budama metodolojisi geliştirildi. Daha sonra aşırı uyumlu ağaç, genelleme doğruluğuna katkıda bulunmayan alt dalları kaldırarak daha küçük bir ağaç haline getirilir. Çeşitli çalışmalarda, budama yöntemlerinin, özellikle gürültülü alanlarda, bir karar ağacının genelleme performansını iyileştirebileceği gösterilmiştir.<br>Budamanın bir diğer önemli motivasyonu da “basitlik için accuracy değiş-tokuşu” dur. Amaç yeterince doğru, kompakt bir konsept tanımı oluşturmak olduğunda, budama(pruning) oldukça faydalıdır. Bu süreç içinde ilk karar ağacı tamamen doğru olarak görüldüğünden, budanmış bir karar ağacının doğruluğu ilk ağaca ne kadar yakın olduğunu gösterir.<br>Karar ağaçlarını budamak için çeşitli teknikler vardır. Çoğu, düğümlerin yukarıdan aşağıya veya aşağıdan yukarıya geçişini gerçekleştirir. Bu işlem belirli bir kriteri iyileştirirse bir düğüm budanır. Aşağıda en popüler teknikleri inceleyelim.</p>
<h4 id="1-Cost-Complexity-Pruning"><a href="#1-Cost-Complexity-Pruning" class="headerlink" title="1.Cost Complexity Pruning"></a>1.Cost Complexity Pruning</h4><p>Cost Complexity Pruning (aynı zamanda en zayıf halka budama veya hata karmaşıklığı budama olarak da bilinir) iki aşamada ilerler. İlk aşamada, T0’ın budama öncesi orijinal ağaç ve Tk’nin kök ağaç olduğu eğitim verileri üzerine T0, T1, …, Tk ağaçları dizisi oluşturulur.</p>
<p>İkinci aşamada bu ağaçlardan biri genelleme hatası tahminine göre budanmış ağaç olarak seçilir. Ti + 1 ağacı, önceki Ti ağacındaki bir veya daha fazla alt ağacın uygun yapraklarla değiştirilmesiyle elde edilir. Budanmış alt ağaçlar, budanmış yaprak başına görünen hata oranındaki en düşük artışı elde edenlerdir:<br><img src="/DecisionTree/18.jpg"><br>Burada ε(T, S), örnek S üzerinden T ağacının hata oranını gösterir ve |leaves (T )| T’deki yaprak sayısını gösterir .<br>pruned (T,t), T’deki t düğümü uygun yaprakla değiştirilerek elde edilen ağacı gösterir.</p>
<p>İkinci aşamada, budanmış her ağacın genelleme hatası T0, T1,…, Tk tahmin edilmektedir. Daha sonra en iyi budanmış ağaç seçilir.</p>
<p>Verilen veri kümesi yeterince büyükse, onu bir eğitim setine ve bir budama setine(pruning set) ayırmak önerilir. Ağaçlar eğitim seti kullanılarak inşa edilir ve budama seti üzerinde değerlendirilir.<br>Öte yandan, verilen veri kümesi yeterince büyük değilse, hesaplama karmaşıklığı etkilerine rağmen çapraz doğrulama(cross-validation) metodolojisinin kullanması öneririlir.</p>
<h4 id="2-Reduced-Error-Pruning"><a href="#2-Reduced-Error-Pruning" class="headerlink" title="2.Reduced Error Pruning"></a>2.Reduced Error Pruning</h4><p>Azaltılmış hata budama olarak bilinen karar ağaçlarını budamak için basit bir prosedür. Alttan üste iç düğümler üzerinde geçiş yaparken, en sık sınıfla değiştirmenin ağacın doğruluğunu azaltıp azaltmadığını belirlemek için her iç düğümü kontrol eder.<br>Accuracy azalmazsa düğüm kesilir. Prosedür, daha fazla budama accuracy azaltana kadar devam eder.<br>Accuracy tahmin etmek için bir budama seti kullanılması önerilir. Bu prosedürün, belirli bir budama setine göre en küçük accuracy alt ağaç ile sona erdiği gösterilebilir.</p>
<h4 id="3-Minimum-Error-Pruning-MEP"><a href="#3-Minimum-Error-Pruning-MEP" class="headerlink" title="3.Minimum Error Pruning (MEP)"></a>3.Minimum Error Pruning (MEP)</h4><p>Minimum hata budama, iç düğümlerin aşağıdan yukarıya geçişini içerir. Bu teknik, her düğümde, budama ile ve budama olmadan l-olasılık hata oranı tahminini karşılaştırır.<br>L-olasılık hata oranı tahmini, frekansları kullanarak basit olasılık tahmininin düzeltilmesidir. St,  yaprak t’ye ulaşan örnekleri gösteriyorsa, bu yapraktaki beklenen hata oranı:</p>
<p><img src="/DecisionTree/19.jpg"></p>
<p>Burada papr (y = ci), y’nin ci değerini aldığı a-priori olasılığıdır ve l, a-priori olasılığına verilen ağırlığı gösterir.</p>
<p>Bir iç düğümün hata oranı, dallarının hata oranının ağırlıklı ortalamasıdır. Ağırlık, her dal boyunca örneklerin oranına göre belirlenir. Hesaplama yapraklara kadar yinelemeli olarak yapılır.</p>
<p>Bir iç düğüm budanırsa, o zaman bir yaprak olur ve hata oranı son denklem kullanılarak doğrudan hesaplanır. Sonuç olarak,<br>belirli bir iç düğümün budamasından önce ve sonra hata oranını karşılaştırabiliriz. Bu düğümün budaması hata oranını arttırmazsa, budama kabul edilmelidir.</p>
<h4 id="4-Pessimistic-Pruning"><a href="#4-Pessimistic-Pruning" class="headerlink" title="4.Pessimistic Pruning"></a>4.Pessimistic Pruning</h4><p>Pessimistic Pruning, bir budama seti veya çapraz doğrulama(cross-validation) ihtiyacını önler ve bunun yerine pessimistic istatistiksel korelasyon testini kullanır.Temel fikir, eğitim seti kullanılarak tahmin edilen hata oranının yeterince güvenilir olmamasıdır. Bunun yerine, binom dağılımı için süreklilik düzeltmesi olarak bilinen daha gerçekçi bir önlem kullanılmalıdır:<br><img src="/DecisionTree/20.jpg"></p>
<p>Ancak, bu düzeltme hala iyimser bir hata oranı üretir. Sonuç olarak, hata oranı bir referans ağacından bir standart hata içinde ise, bir iç düğüm t budama:<br><img src="/DecisionTree/21.jpg"><br>Son koşul, oranlar için istatistiksel güven aralığına dayanmaktadır. Genellikle son koşul, kökü t iç düğüm olan bir alt ağaca karşılık gelir. S, t düğümüne atıfta bulunan eğitim kümesini gösterir.</p>
<h4 id="5-Error-Based-Pruning-EBP"><a href="#5-Error-Based-Pruning-EBP" class="headerlink" title="5.Error-Based Pruning (EBP)"></a>5.Error-Based Pruning (EBP)</h4><p>Error-Based Pruning, kötümser budamanın bir evrimidir. İyi bilinen C4.5 algoritmasında uygulanmaktadır.<br>Pessimistic Pruning’de olduğu gibi, hata oranı, oranlar için istatistiksel güven aralığının üst sınırı kullanılarak tahmin edilir.<br><img src="/DecisionTree/22.jpg"><br>ε(T, S), S eğitim setindeki T ağacının yanlış sınıflandırma oranını gösterir.<br>Z, standart normal kümülatif dağılımın tersidir.<br>α, istenen anlamlılık seviyesidir.<br>Alt ağaç(sub-tree) (T,t), t düğüm tarafından köklendirilen alt ağacı gösterir.<br>maxchild (T, t), t’nin en sık görülen alt düğümünü gösterir.<br>St, t düğümüne ulaşan S’deki tüm örnekleri gösterir.<br>Prosedür, aşağıdan yukarıya tüm düğümleri geçer ve aşağıdaki değerleri karşılaştırır:<br>(1) εUB(subtree(T,t), St)<br>(2) εUB(pruned(subtree(T,t), t), St)<br>(3) εUB(subtree(T, maxchild(T,t)), Smaxchild(T,t))</p>
<p>En düşük değere göre, prosedür ağacı olduğu gibi bırakır,t düğümünü budar veya t düğümünü maxchild(T,t) tarafından köklendirilen alt ağaçla değiştirir.</p>
<h4 id="6-Minimum-Description-Length-MDL-Pruning"><a href="#6-Minimum-Description-Length-MDL-Pruning" class="headerlink" title="6.Minimum Description Length (MDL) Pruning"></a>6.Minimum Description Length (MDL) Pruning</h4><p>Minimum Description Length (MDL), bir düğümün genelleştirilmiş doğruluğunu değerlendirmek için kullanılabilir. Bu yöntem, ağacı kodlamak için gereken bit sayısını kullanarak karar ağacının boyutunu ölçer. MDL yöntemi, daha az bit ile kodlanabilen karar ağaçlarını tercih eder, bir yaprak t’deki bir bölünmenin maliyetinin şu şekilde tahmin edilebileceğini gösterir:<br><img src="/DecisionTree/23.jpg"><br>St, t düğümüne ulaşan örnekleri gösterir. Bir iç düğümün bölme maliyeti, çocuklarının maliyet toplamasına göre hesaplanır.</p>
<h4 id="Comparison-of-Pruning-Methods"><a href="#Comparison-of-Pruning-Methods" class="headerlink" title="Comparison of Pruning Methods"></a>Comparison of Pruning Methods</h4><p>Çeşitli çalışmalar farklı budama tekniklerinin performansını karşılaştırır. Sonuçlar, Cost-Complexity Pruning ve Reduced Error Pruning aşırı budama eğilimindedir, yani daha küçük ama daha az doğru karar ağaçları oluşturur. Error-Based Pruning, Pessimistic Error Pruning ve Minimum Error Pruning düşük budamaya eğilimlidir. Karşılaştırmaların çoğu, <strong>no free lunch teoreminin</strong> budama için de geçerli olduğu sonucuna varmıştır, yani diğer budama yöntemlerinden daha iyi performans gösteren bir budama yöntemi yoktur.</p>
<h3 id="Type-of-Decision-Tree-Algorithm"><a href="#Type-of-Decision-Tree-Algorithm" class="headerlink" title="Type of Decision Tree Algorithm"></a>Type of Decision Tree Algorithm</h3><p>● Classification and Regression Tree (CART)<br>● Iterative Dichotomiser 3 (ID3)<br>● C4.5 and C5.0 (different versions of a robust approach)<br>● Chi-squared Automatic Interaction Detection (CHAID)<br>● Decision Stump<br>● M5<br>● Conditional Decision Trees</p>
<p>En dikkate değer karar ağacı algoritması türleri şunlardır:</p>
<h4 id="Iterative-Dichotomiser-3-ID3"><a href="#Iterative-Dichotomiser-3-ID3" class="headerlink" title="Iterative Dichotomiser 3 (ID3)"></a>Iterative Dichotomiser 3 (ID3)</h4><p>Bu algoritma, hangi özelliğin kullanılacağına karar vermek için Information Gain kullanır ve verilerin mevcut alt kümesini sınıflandırır. Ağacın her seviyesi için, geri kalan veriler için Information Gain özyinelemeli olarak hesaplanır.</p>
<h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4><p>Bu algoritma, ID3 algoritmasının halefidir. Bu algoritma, sınıflandırma niteliğine karar vermek için Information Gain veya Gain ratio kullanır. Hem sürekli hem de eksik nitelik değerlerini işleyebildiği için ID3 algoritmasının doğrudan bir iyileştirmesidir.</p>
<h4 id="Classification-and-Regression-Tree-CART"><a href="#Classification-and-Regression-Tree-CART" class="headerlink" title="Classification and Regression Tree (CART)"></a>Classification and Regression Tree (CART)</h4><p>Bağımlı değişkene bağlı olarak bir regresyon ağacı ve bir sınıflandırma ağacı üretebilen dinamik bir öğrenme algoritmasıdır.</p>
<h3 id="Advantages-of-Decision-Trees"><a href="#Advantages-of-Decision-Trees" class="headerlink" title="Advantages of Decision Trees"></a>Advantages of Decision Trees</h3><ol>
<li>Karar ağaçları kendinden açıklamalıdır ve sıkıştırıldıklarında takip etmeleri de kolaydır. Yani karar ağacında makul sayıda yaprak varsa profesyonel olmayan kullanıcılar tarafından kavranabilir. Dahası, karar ağaçları bir dizi kurala dönüştürülebildiğinden, bu tür bir temsil anlaşılır olarak kabul edilir.</li>
<li>Karar ağaçları, hem nominal hem de sayısal girdi niteliklerini işleyebilir.</li>
<li>Karar ağacı gösterimi, herhangi bir ayrık değer sınıflandırıcısını temsil edecek kadar zengindir.</li>
<li>Karar ağaçları, hatalı olabilecek veri kümelerini işleyebilir.</li>
<li>Karar ağaçları, eksik değerlere sahip olabilecek veri setlerini ele alabilir.</li>
<li>Karar ağaçları, parametrik olmayan bir yöntem olarak kabul edilir, yani kararlar, alan dağılımı ve sınıflandırıcı yapısı hakkında herhangi bir varsayım içermez.</li>
<li>Sınıflandırma maliyeti yüksek olduğunda, karar ağaçları sadece kökten yaprağa tek bir yol boyunca yer alan özelliklerin değerlerini talep etmeleri bakımından çekici olabilir.</li>
</ol>
<h3 id="Disadvantages-of-Decision-Trees"><a href="#Disadvantages-of-Decision-Trees" class="headerlink" title="Disadvantages of Decision Trees"></a>Disadvantages of Decision Trees</h3><ol>
<li>Algoritmaların çoğu (ID3 ve C4.5 gibi), hedef özelliğin yalnızca ayrık değerlere sahip olmasını gerektirir.</li>
<li>Karar ağaçları “böl ve yönet” yöntemini kullandıkça, alaka düzeyi yüksek birkaç özellik varsa iyi performans gösterir, ancak birçok karmaşık etkileşim mevcuttur. Bunun sebeplerinden biri diğer sınıflandırıcıların bir sınıflandırıcıyı kısaca tanımlayabilmesidir.Bunu bir karar ağacı kullanarak temsil etmek çok zor olurdu. Bu fenomenin basit bir örneği, karar ağaçlarının çoğaltma sorunudur. Çoğu karar ağacı, bir kavramı temsil etmek için örnek alanını birbirini dışlayan bölgelere böldüğünden, bazı durumlarda ağaç, sınıflandırıcıyı temsil etmek için aynı alt ağacın birkaç kopyasını içermelidir. Çoğaltma problemi, alt ağaçların tekrarlanmasını ayırıcı kavramlara zorlar.</li>
</ol>
<p>Örneğin, kavram aşağıdaki ikili fonksiyonu takip ederse: y = (A1 ∩ A2) ∪ (A3 ∩ A4) bu fonksiyonu temsil eden minimum tek değişkenli karar ağacı aşağıdaki şekilde gösterilmektedir. Ağacın aynı alt ağacın iki kopyasını içerdiğine dikkat edin.<br><img src="/DecisionTree/16.jpg"></p>
<ol start="3">
<li>Karar ağaçlarının açgözlü özelliği, belirtilmesi gereken başka bir dezavantaja yol açar. Eğitim setine, ilgili niteliklere ve gürültüye aşırı duyarlılık, karar ağaçlarını özellikle istikrarsız hale getirir: köke yakın bir bölünmede küçük bir değişiklik, aşağıdaki tüm alt ağacı değiştirecektir. Eğitim setindeki küçük farklılıklar nedeniyle, algoritma en iyisi olmayan bir özelliği seçebilir.</li>
<li>Parçalanma sorunu, verilerin daha küçük parçalara bölünmesine neden olur. Bu genellikle, yol boyunca birçok özellik test edilirse gerçekleşir. Veriler her bölünmede yaklaşık olarak eşit olarak bölünürse, tek değişkenli bir karar ağacı O (logn) özelliklerinden fazlasını test edemez. Bu, birçok ilgili özelliğe sahip görevler için karar ağaçlarını dezavantajlı konuma getirir. Çoğaltmanın her zaman parçalanmayı ifade ettiğini, ancak parçalanmanın herhangi bir çoğaltma olmadan gerçekleşebileceğini unutmayın.</li>
<li>Başka bir sorun, eksik değerlerle başa çıkmak için gereken çabayla ilgilidir. Eksik değerleri ele alma becerisi bir avantaj olarak kabul edilirken, bunu başarmak için gereken aşırı çaba bir dezavantaj olarak kabul edilir. Test edilen bir özellik eksikse alınacak doğru dal bilinmemektedir ve algoritmanın eksik değerleri işlemek için özel mekanizmalar kullanması gerekir. C4.5, eksik değerler üzerindeki test oluşumlarını azaltmak için, information gaini  bilinmeyen vakaların oranıyla cezalandırır ve ardından bu örnekleri alt ağaçlara böler. CART, çok daha karmaşık bir vekil özellikler(feature) şeması kullanır.</li>
<li>Karar ağacı indüksiyon algoritmalarının çoğunun miyop doğası indükleyicilerin yalnızca bir seviye ileriye bakması gerçeğiyle yansıtılır. Özellikle, bölme kriteri, olası öznitelikleri hemen soyundan gelenlere göre sıralar. Bu tür bir strateji, tek başına yüksek puan alan testleri tercih eder ve özellik kombinasyonlarını gözden kaçırabilir. Daha derin önden bakış stratejilerinin kullanılması, hesaplama açısından pahalı kabul edilir ve yararlı olduğu kanıtlanmamıştır.</li>
</ol>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Decision-Tree/">Decision Tree</a><a class="link-muted mr-2" rel="tag" href="/tags/Information-Gain/">Information Gain</a><a class="link-muted mr-2" rel="tag" href="/tags/Gini-Index/">Gini Index</a><a class="link-muted mr-2" rel="tag" href="/tags/Entropy/">Entropy</a><a class="link-muted mr-2" rel="tag" href="/tags/Gini-Impurity/">Gini Impurity</a><a class="link-muted mr-2" rel="tag" href="/tags/Pruning-Tree/">Pruning Tree</a><a class="link-muted mr-2" rel="tag" href="/tags/Cost-Complexity-Pruning/">Cost Complexity Pruning</a><a class="link-muted mr-2" rel="tag" href="/tags/Reduced-Error-Pruning/">Reduced Error Pruning</a><a class="link-muted mr-2" rel="tag" href="/tags/Minimum-Error-Pruning-MEP/">Minimum Error Pruning (MEP)</a><a class="link-muted mr-2" rel="tag" href="/tags/Pessimistic-Pruning/">Pessimistic Pruning</a><a class="link-muted mr-2" rel="tag" href="/tags/Error-Based-Pruning-EBP/">Error-Based Pruning (EBP)</a><a class="link-muted mr-2" rel="tag" href="/tags/Minimum-Description-Length-MDL-Pruning/">Minimum Description Length (MDL) Pruning</a><a class="link-muted mr-2" rel="tag" href="/tags/Iterative-Dichotomiser-3-ID3/">Iterative Dichotomiser 3 (ID3)</a><a class="link-muted mr-2" rel="tag" href="/tags/C4-5/">C4.5</a><a class="link-muted mr-2" rel="tag" href="/tags/Classification-and-Regression-Tree-CART/">Classification and Regression Tree (CART)</a></div><link rel="stylesheet" href="/css/share.css"><div><a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?text=Decision Tree Algorithm&amp;url=https://kaderdurak.github.io//DecisionTree/" target="_blank" rel="noopener" aria-label=""><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div class="resp-sharing-button__icon resp-sharing-button__icon--solidcircle" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 0C5.38 0 0 5.38 0 12s5.38 12 12 12 12-5.38 12-12S18.62 0 12 0zm5.26 9.38v.34c0 3.48-2.64 7.5-7.48 7.5-1.48 0-2.87-.44-4.03-1.2 1.37.17 2.77-.2 3.9-1.08-1.16-.02-2.13-.78-2.46-1.83.38.1.8.07 1.17-.03-1.2-.24-2.1-1.3-2.1-2.58v-.05c.35.2.75.32 1.18.33-.7-.47-1.17-1.28-1.17-2.2 0-.47.13-.92.36-1.3C7.94 8.85 9.88 9.9 12.06 10c-.04-.2-.06-.4-.06-.6 0-1.46 1.18-2.63 2.63-2.63.76 0 1.44.3 1.92.82.6-.12 1.95-.27 1.95-.27-.35.53-.72 1.66-1.24 2.04z"></path></svg></div></div></a><a class="resp-sharing-button__link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://kaderdurak.github.io//DecisionTree/&amp;title=Decision Tree Algorithm&amp;source=https://kaderdurak.github.io//DecisionTree/" target="_blank" rel="noopener" aria-label=""><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div class="resp-sharing-button__icon resp-sharing-button__icon--solidcircle" aria-hidden="true"><svg version="1.1" x="0px" y="0px" width="24px" height="24px" viewBox="0 0 24 24" enable-background="new 0 0 24 24"><path d="M12,0C5.383,0,0,5.383,0,12s5.383,12,12,12s12-5.383,12-12S18.617,0,12,0z M9.5,16.5h-2v-7h2V16.5z M8.5,7.5 c-0.553,0-1-0.448-1-1c0-0.552,0.447-1,1-1s1,0.448,1,1C9.5,7.052,9.053,7.5,8.5,7.5z M18.5,16.5h-3V13c0-0.277-0.225-0.5-0.5-0.5 c-0.276,0-0.5,0.223-0.5,0.5v3.5h-3c0,0,0.031-6.478,0-7h3v0.835c0,0,0.457-0.753,1.707-0.753c1.55,0,2.293,1.12,2.293,3.296V16.5z"></path></svg></div></div></a><a class="resp-sharing-button__link" href="mailto:?subject=Decision Tree Algorithm&amp;body=Bunu mutlaka okumalısın. https://kaderdurak.github.io//DecisionTree/" target="_self" rel="noopener" aria-label=""><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div class="resp-sharing-button__icon resp-sharing-button__icon--solidcircle" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 0C5.38 0 0 5.38 0 12s5.38 12 12 12 12-5.38 12-12S18.62 0 12 0zm8 16c0 1.1-.9 2-2 2H6c-1.1 0-2-.9-2-2V8c0-1.1.9-2 2-2h12c1.1 0 2 .9 2 2v8z"></path><path d="M17.9 8.18c-.2-.2-.5-.24-.72-.07L12 12.38 6.82 8.1c-.22-.16-.53-.13-.7.08s-.15.53.06.7l3.62 2.97-3.57 2.23c-.23.14-.3.45-.15.7.1.14.25.22.42.22.1 0 .18-.02.27-.08l3.85-2.4 1.06.87c.1.04.2.1.32.1s.23-.06.32-.1l1.06-.9 3.86 2.4c.08.06.17.1.26.1.17 0 .33-.1.42-.25.15-.24.08-.55-.15-.7l-3.57-2.22 3.62-2.96c.2-.2.24-.5.07-.72z"></path></svg></div></div></a><a class="resp-sharing-button__link" href="whatsapp://send?text=Decision Tree Algorithmhttps://kaderdurak.github.io//DecisionTree/" target="_blank" rel="noopener" aria-label=""><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div class="resp-sharing-button__icon resp-sharing-button__icon--solidcircle" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 24 24"><path d="m12 0c-6.6 0-12 5.4-12 12s5.4 12 12 12 12-5.4 12-12-5.4-12-12-12zm0 3.8c2.2 0 4.2 0.9 5.7 2.4 1.6 1.5 2.4 3.6 2.5 5.7 0 4.5-3.6 8.1-8.1 8.1-1.4 0-2.7-0.4-3.9-1l-4.4 1.1 1.2-4.2c-0.8-1.2-1.1-2.6-1.1-4 0-4.5 3.6-8.1 8.1-8.1zm0.1 1.5c-3.7 0-6.7 3-6.7 6.7 0 1.3 0.3 2.5 1 3.6l0.1 0.3-0.7 2.4 2.5-0.7 0.3 0.099c1 0.7 2.2 1 3.4 1 3.7 0 6.8-3 6.9-6.6 0-1.8-0.7-3.5-2-4.8s-3-2-4.8-2zm-3 2.9h0.4c0.2 0 0.4-0.099 0.5 0.3s0.5 1.5 0.6 1.7 0.1 0.2 0 0.3-0.1 0.2-0.2 0.3l-0.3 0.3c-0.1 0.1-0.2 0.2-0.1 0.4 0.2 0.2 0.6 0.9 1.2 1.4 0.7 0.7 1.4 0.9 1.6 1 0.2 0 0.3 0.001 0.4-0.099s0.5-0.6 0.6-0.8c0.2-0.2 0.3-0.2 0.5-0.1l1.4 0.7c0.2 0.1 0.3 0.2 0.5 0.3 0 0.1 0.1 0.5-0.099 1s-1 0.9-1.4 1c-0.3 0-0.8 0.001-1.3-0.099-0.3-0.1-0.7-0.2-1.2-0.4-2.1-0.9-3.4-3-3.5-3.1s-0.8-1.1-0.8-2.1c0-1 0.5-1.5 0.7-1.7s0.4-0.3 0.5-0.3z"></path></svg></div></div></a><a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Decision Tree Algorithm&amp;url=https://kaderdurak.github.io//DecisionTree/" target="_blank" rel="noopener" aria-label=""><div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div class="resp-sharing-button__icon resp-sharing-button__icon--solidcircle" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 23.5c6.35 0 11.5-5.15 11.5-11.5S18.35.5 12 .5.5 5.65.5 12 5.65 23.5 12 23.5zM2.505 11.053c-.31.118-.505.738-.505.738s.203.62.513.737l3.636 1.355 1.417 4.557a.787.787 0 0 0 1.25.375l2.115-1.72a.29.29 0 0 1 .353-.01L15.1 19.85a.786.786 0 0 0 .746.095.786.786 0 0 0 .487-.573l2.793-13.426a.787.787 0 0 0-1.054-.893l-15.568 6z" fill-rule="evenodd"></path></svg></div></div></a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/RealTime-IOT-Project/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Real Time Iot Project</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/NaiveBayes/"><span class="level-item">Naive Bayes Classifier</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Kader Durak"></figure><p class="title is-size-4 is-block line-height-inherit">Kader Durak</p><p class="is-size-6 is-block">Data Science Enthusiast</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>İstanbul</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Gönderiler</p><a href="/archives"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Kategoriler</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Etiketler</p><a href="/tags"><p class="title">83</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KaderDurak" target="_blank" rel="noopener">TAKİP ET</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KaderDurak/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="HackerRank" href="http://hackerrank.com/"><i class="fab fa-hackerrank"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Kaggle" href="https://www.kaggle.com/"><i class="fab fa-kaggle"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="http://medium.com/"><i class="fab fa-medium"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Faydalı Linkler</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://datacamp.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">DataCamp</span></span><span class="level-right"><span class="level-item tag">datacamp.com</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://coursera.org/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Coursera</span></span><span class="level-right"><span class="level-item tag">coursera.org</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://udemy.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Udemy</span></span><span class="level-right"><span class="level-item tag">udemy.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Arşivler</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">Şubat 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/01/"><span class="level-start"><span class="level-item">Ocak 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">Aralık 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">Kasım 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">Ekim 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">Eylül 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">Ağustos 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">Temmuz 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Kategoriler</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Son</h3><article class="media"><a class="media-left" href="/RealTime-IOT-Project/"><p class="image is-64x64"><img class="thumbnail" src="/RealTime-IOT-Project%5Ciot.png" alt="Real Time Iot Project"></p></a><div class="media-content size-small"><p><time dateTime="2021-02-10T21:24:26.000Z">2021-02-11</time></p><p class="title is-6"><a class="link-muted" href="/RealTime-IOT-Project/">Real Time Iot Project</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><a class="media-left" href="/DecisionTree/"><p class="image is-64x64"><img class="thumbnail" src="/DecisionTree/EoapOPlXcAE_XPa.jpg" alt="Decision Tree Algorithm"></p></a><div class="media-content size-small"><p><time dateTime="2021-01-11T19:11:38.000Z">2021-01-11</time></p><p class="title is-6"><a class="link-muted" href="/DecisionTree/">Decision Tree Algorithm</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><a class="media-left" href="/NaiveBayes/"><p class="image is-64x64"><img class="thumbnail" src="/NaiveBayes/ricky.png" alt="Naive Bayes Classifier"></p></a><div class="media-content size-small"><p><time dateTime="2020-12-13T19:32:54.000Z">2020-12-13</time></p><p class="title is-6"><a class="link-muted" href="/NaiveBayes/">Naive Bayes Classifier</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><a class="media-left" href="/SVM/"><p class="image is-64x64"><img class="thumbnail" src="/SVM/bas.png" alt="Support Vector Machines (SVMs)"></p></a><div class="media-content size-small"><p><time dateTime="2020-12-03T08:42:30.000Z">2020-12-03</time></p><p class="title is-6"><a class="link-muted" href="/SVM/">Support Vector Machines (SVMs)</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><a class="media-left" href="/Algorithm/"><p class="image is-64x64"><img class="thumbnail" src="/Algorithm/aaa.jpg" alt="Debugging Learning Algorithm"></p></a><div class="media-content size-small"><p><time dateTime="2020-11-14T17:02:46.000Z">2020-11-14</time></p><p class="title is-6"><a class="link-muted" href="/Algorithm/">Debugging Learning Algorithm</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Etiketler</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AUC/"><span class="tag">AUC</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Accuracy/"><span class="tag">Accuracy</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Artificial-Neural-Networks-ANNs/"><span class="tag">Artificial Neural Networks (ANNs)</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Back-Propagation-Algorithm/"><span class="tag">Back Propagation Algorithm</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bayes-Networks/"><span class="tag">Bayes Networks</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bayes-Theorem/"><span class="tag">Bayes Theorem</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bernoulli-Naive-Bayes/"><span class="tag">Bernoulli Naive Bayes</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bias/"><span class="tag">Bias</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Data-Engineer/"><span class="tag">Big Data Engineer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Classification/"><span class="tag">Binary Classification</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C4-5/"><span class="tag">C4.5</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification-and-Regression-Tree-CART/"><span class="tag">Classification and Regression Tree (CART)</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cost-Complexity-Pruning/"><span class="tag">Cost Complexity Pruning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CostFunction/"><span class="tag">CostFunction</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Validation/"><span class="tag">Cross Validation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Decision-Boundary/"><span class="tag">Decision Boundary</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Decision-Tree/"><span class="tag">Decision Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Directed-Acyclic-Graphs-DAGs/"><span class="tag">Directed Acyclic Graphs (DAGs)</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Entropy/"><span class="tag">Entropy</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Error-Analysis/"><span class="tag">Error Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Error-Based-Pruning-EBP/"><span class="tag">Error-Based Pruning (EBP)</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/F1-Score/"><span class="tag">F1 Score</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Feature-Scaling/"><span class="tag">Feature Scaling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeedBack-Neural-Network/"><span class="tag">FeedBack Neural Network</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeedForward-Neural-Networks/"><span class="tag">FeedForward Neural Networks</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT-3/"><span class="tag">GPT-3</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gaussian-Naive-Bayes/"><span class="tag">Gaussian Naive Bayes</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gini-Impurity/"><span class="tag">Gini Impurity</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gini-Index/"><span class="tag">Gini Index</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Grafana/"><span class="tag">Grafana</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hidden-Layer/"><span class="tag">Hidden Layer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Information-Gain/"><span class="tag">Information Gain</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Input-Layer/"><span class="tag">Input Layer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iot/"><span class="tag">Iot</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iterative-Dichotomiser-3-ID3/"><span class="tag">Iterative Dichotomiser 3 (ID3)</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kafka/"><span class="tag">Kafka</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kernels-Tricks/"><span class="tag">Kernels Tricks</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lagrange-Multipliers/"><span class="tag">Lagrange Multipliers</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learning-Curve/"><span class="tag">Learning Curve</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Regression/"><span class="tag">Linear Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Logistic-Regression/"><span class="tag">Logistic Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Max-Abs-Scaler/"><span class="tag">Max Abs Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Min-Max-Scaler/"><span class="tag">Min-Max Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Minimum-Description-Length-MDL-Pruning/"><span class="tag">Minimum Description Length (MDL) Pruning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Minimum-Error-Pruning-MEP/"><span class="tag">Minimum Error Pruning (MEP)</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model/"><span class="tag">Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Evaluation-Metrics/"><span class="tag">Model Evaluation Metrics</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Selection/"><span class="tag">Model Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mosquitto/"><span class="tag">Mosquitto</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multiclass-Classification/"><span class="tag">Multiclass Classification</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multinomial-Naive-Bayes/"><span class="tag">Multinomial Naive Bayes</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Naive-Bayes/"><span class="tag">Naive Bayes</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenAI/"><span class="tag">OpenAI</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Output-Layer/"><span class="tag">Output Layer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Overfitting/"><span class="tag">Overfitting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pessimistic-Pruning/"><span class="tag">Pessimistic Pruning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Polynomial-Regression/"><span class="tag">Polynomial Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Postgres-Sql/"><span class="tag">Postgres Sql</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Power-Transformer-Scaler/"><span class="tag">Power Transformer Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Precision/"><span class="tag">Precision</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probability/"><span class="tag">Probability</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pruning-Tree/"><span class="tag">Pruning Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Quantile-Transformer-Scaler/"><span class="tag">Quantile Transformer Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROC/"><span class="tag">ROC</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real-Time-Analysis/"><span class="tag">Real Time Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recall/"><span class="tag">Recall</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reduced-Error-Pruning/"><span class="tag">Reduced Error Pruning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation/"><span class="tag">Representation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Robust-Scaler/"><span class="tag">Robust Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Smart-Agriculture/"><span class="tag">Smart Agriculture</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Smart-City/"><span class="tag">Smart City</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Smart-House/"><span class="tag">Smart House</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Soft-Margin-Formulation/"><span class="tag">Soft Margin Formulation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spark-Streamming/"><span class="tag">Spark Streamming</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Standard-Scaler/"><span class="tag">Standard Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Support-Vector-Machine/"><span class="tag">Support Vector Machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Underfitting/"><span class="tag">Underfitting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unit-Vector-Scaler/"><span class="tag">Unit Vector Scaler</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variance/"><span class="tag">Variance</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Zookeeper/"><span class="tag">Zookeeper</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KADER DURAK BLOG" height="28"></a><p class="size-small"><span>&copy; 2021 Kader Durak</span></p></div><p>Yazara ait blog yazıları eğitim amaçlıdır.<br>i&#039;m cyborg but that&#039;s ok</p><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("tr");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://kaderdurak.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Zurück nach oben" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Bir şeyler yaz..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Bir şeyler yaz...","untitled":"(Untitled)","posts":"Gönderiler","pages":"Pages","categories":"Kategoriler","tags":"Etiketler"});
        });</script></body></html>